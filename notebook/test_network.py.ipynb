{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get a data batch firstly\n",
    "'''\n",
    "import os,sys\n",
    "sys.path.append('../')\n",
    "from data.data_loader import PUNET_Dataset\n",
    "dataset=PUNET_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_input,gt,radius=dataset.__getitem__(0)\n",
    "pcd_input2,gt2,radius2=dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Conv1d,Conv2d\n",
    "from knn_cuda import KNN\n",
    "from pointnet2.utils.pointnet2_utils import gather_operation,grouping_operation\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class get_edge_feature(nn.Module):\n",
    "    \"\"\"construct edge feature for each point\n",
    "    Args:\n",
    "        tensor: input a point cloud tensor,batch_size,num_dims,num_points\n",
    "        k: int\n",
    "    Returns:\n",
    "        edge features: (batch_size,num_dims,num_points,k)\n",
    "    \"\"\"\n",
    "    def __init__(self,k=16):\n",
    "        super(get_edge_feature,self).__init__()\n",
    "        self.KNN=KNN(k=k+1,transpose_mode=False)\n",
    "        self.k=k\n",
    "    def forward(self,point_cloud):\n",
    "        dist,idx=self.KNN(point_cloud,point_cloud)\n",
    "        '''\n",
    "        idx is batch_size,k,n_points\n",
    "        point_cloud is batch_size,n_dims,n_points\n",
    "        point_cloud_neightbors is batch_size,n_dims,k,n_points\n",
    "        '''\n",
    "        idx=idx[:,1:,:]\n",
    "        point_cloud_neighbors=grouping_operation(point_cloud,idx.contiguous().int())\n",
    "        point_cloud_central=point_cloud.unsqueeze(2).repeat(1,1,self.k,1)\n",
    "        #print(point_cloud_central.shape,point_cloud_neighbors.shape)\n",
    "        edge_feature=torch.cat([point_cloud_central,point_cloud_neighbors-point_cloud_central],dim=1)\n",
    "\n",
    "        return edge_feature,idx\n",
    "\n",
    "\n",
    "\n",
    "        return dist,idx\n",
    "\n",
    "class denseconv(nn.Module):\n",
    "    def __init__(self,growth_rate=64,k=16,in_channels=6,isTrain=True):\n",
    "        super(denseconv,self).__init__()\n",
    "        self.edge_feature_model=get_edge_feature(k=k)\n",
    "        '''\n",
    "        input to conv1 is batch_size,2xn_dims,k,n_points\n",
    "        '''\n",
    "        self.conv1=nn.Sequential(\n",
    "            Conv2d(in_channels=in_channels,out_channels=growth_rate,kernel_size=[1,1]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            Conv2d(in_channels=growth_rate+in_channels,out_channels=growth_rate,kernel_size=[1,1]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv3=nn.Sequential(\n",
    "            Conv2d(in_channels=2*growth_rate+in_channels,out_channels=growth_rate,kernel_size=[1,1]),\n",
    "        )\n",
    "    def forward(self,input):\n",
    "        '''\n",
    "        y should be batch_size,in_channel,k,n_points\n",
    "        '''\n",
    "        y,idx=self.edge_feature_model(input)\n",
    "        inter_result=torch.cat([self.conv1(y),y],dim=1) #concat on feature dimension\n",
    "        inter_result=torch.cat([self.conv2(inter_result),inter_result],dim=1)\n",
    "        inter_result=torch.cat([self.conv3(inter_result),inter_result],dim=1)\n",
    "        final_result=torch.max(inter_result,dim=2)[0] #pool the k channel\n",
    "        return final_result,idx\n",
    "\n",
    "\n",
    "class feature_extraction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(feature_extraction,self).__init__()\n",
    "        self.growth_rate=24\n",
    "        self.dense_n=3\n",
    "        self.knn=16\n",
    "        self.input_channel=3\n",
    "        comp=self.growth_rate*2\n",
    "        '''\n",
    "        make sure to permute the input, the feature dimension is in the second one.\n",
    "        input of conv1 is batch_size,num_dims,num_points\n",
    "        '''\n",
    "        self.conv1=nn.Sequential(\n",
    "            Conv1d(in_channels=self.input_channel,out_channels=24,kernel_size=1,padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.denseconv1=denseconv(in_channels=24*2,growth_rate=self.growth_rate)#return batch_size,(3*24+48)=120,num_points\n",
    "        self.conv2=nn.Sequential(\n",
    "            Conv1d(in_channels=144,out_channels=comp,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.denseconv2=denseconv(in_channels=comp*2,growth_rate=self.growth_rate)\n",
    "        self.conv3=nn.Sequential(\n",
    "            Conv1d(in_channels=312,out_channels=comp,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.denseconv3=denseconv(in_channels=comp*2,growth_rate=self.growth_rate)\n",
    "        self.conv4=nn.Sequential(\n",
    "            Conv1d(in_channels=480,out_channels=comp,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.denseconv4=denseconv(in_channels=comp*2,growth_rate=self.growth_rate)\n",
    "    def forward(self,input):\n",
    "        l0_features=self.conv1(input) #b,24,n\n",
    "        #print(l0_features.shape)\n",
    "        l1_features,l1_index=self.denseconv1(l0_features) #b,24*2+24*3=120,n\n",
    "        l1_features=torch.cat([l1_features,l0_features],dim=1) #b,120+24=144,n\n",
    "\n",
    "        l2_features=self.conv2(l1_features) #b,48,n\n",
    "        l2_features,l2_index=self.denseconv2(l2_features) #b,48*2+24*3=168,n\n",
    "        l2_features=torch.cat([l2_features,l1_features],dim=1)#b,168+144=312,n\n",
    "\n",
    "        l3_features=self.conv3(l2_features)#b,48,n\n",
    "        l3_features,l3_index=self.denseconv3(l3_features)#b,48*2+24*3=168,n\n",
    "        l3_features=torch.cat([l3_features,l2_features],dim=1)#b,168+312=480,n\n",
    "\n",
    "        l4_features=self.conv4(l3_features)#b,48,n\n",
    "        l4_features,l4_index=self.denseconv4(l4_features)\n",
    "        l4_features=torch.cat([l4_features,l3_features],dim=1)#b,168+480=648,n\n",
    "\n",
    "        return l4_features\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,params):\n",
    "        super(Generator,self).__init__()\n",
    "        self.feature_extractor=feature_extraction()\n",
    "        self.up_ratio=params['up_ratio']\n",
    "        self.num_points=params['patch_num_point']\n",
    "        self.out_num_point=int(self.num_points*self.up_ratio)\n",
    "        self.up_projection_unit=up_projection_unit()\n",
    "\n",
    "        self.conv1=nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128,out_channels=64,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64,out_channels=3,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,input):\n",
    "        features=self.feature_extractor(input) #b,648,n\n",
    "\n",
    "\n",
    "        H=self.up_projection_unit(features) #b,128,4*n\n",
    "\n",
    "        coord=self.conv1(H)\n",
    "        coord=self.conv2(coord)\n",
    "        return coord\n",
    "    def set_requires_grad(self, nets, requires_grad=False):\n",
    "        if not isinstance(nets, list):\n",
    "            nets = [nets]\n",
    "        for net in nets:\n",
    "            if net is not None:\n",
    "                for param in net.parameters():\n",
    "                    param.requires_grad = requires_grad\n",
    "\n",
    "class attention_unit(nn.Module):\n",
    "    def __init__(self,in_channels=130):\n",
    "        super(attention_unit,self).__init__()\n",
    "        self.convF=nn.Sequential(\n",
    "            Conv1d(in_channels=in_channels,out_channels=in_channels//4,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.convG = nn.Sequential(\n",
    "            Conv1d(in_channels=in_channels, out_channels=in_channels// 4, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.convH = nn.Sequential(\n",
    "            Conv1d(in_channels=in_channels, out_channels=in_channels, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.gamma=Variable(torch.zeros([1])).cuda()\n",
    "    def forward(self,inputs):\n",
    "        f=self.convF(inputs)\n",
    "        g=self.convG(inputs)#b,32,n\n",
    "        h=self.convH(inputs)\n",
    "        s=torch.matmul(g.permute(0,2,1),f)#b,n,n\n",
    "        beta=F.softmax(s,dim=2)#b,n,n\n",
    "\n",
    "        o=torch.matmul(h,beta)#b,130,n\n",
    "\n",
    "        x=self.gamma*o+inputs\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class up_block(nn.Module):\n",
    "    def __init__(self,up_ratio=4,in_channels=130):\n",
    "        super(up_block,self).__init__()\n",
    "        self.up_ratio=up_ratio\n",
    "        self.conv1=nn.Sequential(\n",
    "            Conv1d(in_channels=in_channels,out_channels=256,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            Conv1d(in_channels=256,out_channels=128,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.grid=torch.tensor(self.gen_grid(up_ratio)).cuda()\n",
    "        self.attention_unit=attention_unit(in_channels=in_channels)\n",
    "    def forward(self,inputs):\n",
    "        net=inputs #b,128,n\n",
    "        grid=self.grid.clone()\n",
    "        grid=grid.unsqueeze(0).repeat(net.shape[0],1,net.shape[2])#b,4,2*n\n",
    "        grid=grid.view([net.shape[0],-1,2])#b,4*n,2\n",
    "        grid=grid.permute(0,2,1)#b,2,4*n\n",
    "\n",
    "        net=net.repeat(1,1,self.up_ratio)#b,128,n*4\n",
    "\n",
    "        net=torch.cat([net,grid],dim=1)#b,130,n*4\n",
    "\n",
    "        net=self.attention_unit(net)\n",
    "\n",
    "        net=self.conv1(net)\n",
    "        net=self.conv2(net)\n",
    "\n",
    "        return net\n",
    "\n",
    "\n",
    "    def gen_grid(self,up_ratio):\n",
    "        import math\n",
    "        sqrted=int(math.sqrt(up_ratio))+1\n",
    "        for i in range(1,sqrted+1).__reversed__():\n",
    "            if (up_ratio%i)==0:\n",
    "                num_x=i\n",
    "                num_y=up_ratio//i\n",
    "                break\n",
    "        grid_x=torch.linspace(-0.2,0.2,num_x)\n",
    "        grid_y=torch.linspace(-0.2,0.2,num_y)\n",
    "\n",
    "        x,y=torch.meshgrid([grid_x,grid_y])\n",
    "        grid=torch.stack([x,y],dim=-1)#2,2,2\n",
    "        grid=grid.view([-1,2])#4,2\n",
    "        return grid\n",
    "\n",
    "class down_block(nn.Module):\n",
    "    def __init__(self,up_ratio=4,in_channels=128):\n",
    "        super(down_block,self).__init__()\n",
    "        self.conv1=nn.Sequential(\n",
    "            Conv2d(in_channels=in_channels,out_channels=256,kernel_size=[up_ratio,1],padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            Conv1d(in_channels=256,out_channels=128,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up_ratio=up_ratio\n",
    "    def forward(self,inputs):\n",
    "        net=inputs#b,128,n*4\n",
    "        net=net.view([inputs.shape[0],inputs.shape[1],self.up_ratio,-1])#b,128,4,n\n",
    "        net=self.conv1(net)#b,256,1,n\n",
    "        net=net.squeeze(2)\n",
    "        net=self.conv2(net)\n",
    "        return net\n",
    "\n",
    "\n",
    "class up_projection_unit(nn.Module):\n",
    "    def __init__(self,up_ratio=4):\n",
    "        super(up_projection_unit,self).__init__()\n",
    "        self.conv1=nn.Sequential(\n",
    "            Conv1d(in_channels=648,out_channels=128,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up_block1=up_block(up_ratio=4,in_channels=128+2)\n",
    "        self.up_block2=up_block(up_ratio=4,in_channels=128+2)\n",
    "        self.down_block=down_block(up_ratio=4,in_channels=128)\n",
    "    def forward(self,input):\n",
    "        L=self.conv1(input)#b,128,n\n",
    "\n",
    "        H0=self.up_block1(L)#b,128,n*4\n",
    "        L0=self.down_block(H0)#b,128,n\n",
    "\n",
    "        E0=L0-L #b,128,n\n",
    "        H1=self.up_block2(E0)#b,128,4*n\n",
    "        H2=H0+H1 #b,128,4*n\n",
    "        return H2\n",
    "\n",
    "class mlp_conv(nn.Module):\n",
    "    def __init__(self,in_channels,layer_dim):\n",
    "        super(mlp_conv,self).__init__()\n",
    "        self.conv_list=nn.ModuleList()\n",
    "        for i,num_out_channel in enumerate(layer_dim[:-1]):\n",
    "            if i==0:\n",
    "                sub_module=nn.Sequential(\n",
    "                    Conv1d(in_channels=in_channels, out_channels=num_out_channel, kernel_size=1),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                self.conv_list.append(sub_module)\n",
    "            else:\n",
    "                sub_module=nn.Sequential(\n",
    "                    Conv1d(in_channels=layer_dim[i-1],out_channels=num_out_channel,kernel_size=1),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "        self.conv_list.append(\n",
    "            Conv1d(in_channels=layer_dim[-2],out_channels=layer_dim[-1],kernel_size=1)\n",
    "        )\n",
    "    def forward(self,inputs):\n",
    "        net=inputs\n",
    "        for module in self.conv_list:\n",
    "            net=module(net)\n",
    "        return net\n",
    "\n",
    "class mlp(nn.Module):\n",
    "    def __init__(self,in_channels,layer_dim):\n",
    "        super(mlp,self).__init__()\n",
    "        self.mlp_list=nn.ModuleList()\n",
    "        for i,num_outputs in enumerate(layer_dim[:-1]):\n",
    "            if i==0:\n",
    "                sub_module=nn.Sequential(\n",
    "                    nn.Linear(in_channels, num_outputs),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                self.mlp_list.append(sub_module)\n",
    "            else:\n",
    "                sub_module=nn.Sequential(\n",
    "                    nn.Linear(layer_dim[i-1],num_outputs),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                self.mlp_list.append(sub_module)\n",
    "        self.mlp_list.append(\n",
    "            nn.Linear(layer_dim[-2],layer_dim[-1])\n",
    "        )\n",
    "    def forward(self,inputs):\n",
    "        net=inputs\n",
    "        for sub_module in self.mlp_list:\n",
    "            net=sub_module(net)\n",
    "        return net\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,params,in_channels):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.params=params\n",
    "        self.start_number=32\n",
    "        self.mlp_conv1=mlp_conv(in_channels=in_channels,layer_dim=[self.start_number, self.start_number * 2])\n",
    "        self.attention_unit=attention_unit(in_channels=self.start_number*4)\n",
    "        self.mlp_conv2=mlp_conv(in_channels=self.start_number*4,layer_dim=[self.start_number*4,self.start_number*8])\n",
    "        self.mlp=mlp(in_channels=self.start_number*8,layer_dim=[self.start_number * 8, 1])\n",
    "    def forward(self,inputs):\n",
    "        features=self.mlp_conv1(inputs)\n",
    "        features_global=torch.max(features,dim=2)[0] ##global feature\n",
    "        features=torch.cat([features,features_global.unsqueeze(2).repeat(1,1,features.shape[2])],dim=1)\n",
    "        features=self.attention_unit(features)\n",
    "\n",
    "        features=self.mlp_conv2(features)\n",
    "        features=torch.max(features,dim=2)[0]\n",
    "\n",
    "        output=self.mlp(features)\n",
    "\n",
    "        return output\n",
    "    def set_requires_grad(self, nets, requires_grad=False):\n",
    "        if not isinstance(nets, list):\n",
    "            nets = [nets]\n",
    "        for net in nets:\n",
    "            if net is not None:\n",
    "                for param in net.parameters():\n",
    "                    param.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This sesstion is for testing the feature extractor\n",
    "'''\n",
    "class feature_extraction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(feature_extraction,self).__init__()\n",
    "        self.growth_rate=24\n",
    "        self.dense_n=3\n",
    "        self.knn=16\n",
    "        self.input_channel=3\n",
    "        comp=self.growth_rate*2\n",
    "        '''\n",
    "        make sure to permute the input, the feature dimension is in the second one.\n",
    "        input of conv1 is batch_size,num_dims,num_points\n",
    "        '''\n",
    "        self.conv1=nn.Sequential(\n",
    "            Conv1d(in_channels=self.input_channel,out_channels=24,kernel_size=1,padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.denseconv1=denseconv(in_channels=24*2,growth_rate=self.growth_rate)#return batch_size,(3*24+48)=120,num_points\n",
    "        self.conv2=nn.Sequential(\n",
    "            Conv1d(in_channels=144,out_channels=comp,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.denseconv2=denseconv(in_channels=comp*2,growth_rate=self.growth_rate)\n",
    "        self.conv3=nn.Sequential(\n",
    "            Conv1d(in_channels=312,out_channels=comp,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.denseconv3=denseconv(in_channels=comp*2,growth_rate=self.growth_rate)\n",
    "        self.conv4=nn.Sequential(\n",
    "            Conv1d(in_channels=480,out_channels=comp,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.denseconv4=denseconv(in_channels=comp*2,growth_rate=self.growth_rate)\n",
    "    def forward(self,input):\n",
    "        l0_features=self.conv1(input) #b,24,n\n",
    "        print(l0_features.shape)\n",
    "        l1_features,l1_index=self.denseconv1(l0_features) #b,24*2+24*3=120,n\n",
    "        l1_features=torch.cat([l1_features,l0_features],dim=1) #b,120+24=144,n\n",
    "        print(l1_features.shape)\n",
    "\n",
    "        l2_features=self.conv2(l1_features) #b,48,n\n",
    "        l2_features,l2_index=self.denseconv2(l2_features) #b,48*2+24*3=168,n\n",
    "        l2_features=torch.cat([l2_features,l1_features],dim=1)#b,168+144=312,n\n",
    "        print(l2_features.shape)\n",
    "\n",
    "        l3_features=self.conv3(l2_features)#b,48,n\n",
    "        l3_features,l3_index=self.denseconv3(l3_features)#b,48*2+24*3=168,n\n",
    "        l3_features=torch.cat([l3_features,l2_features],dim=1)#b,168+312=480,n\n",
    "        print(l3_features.shape)\n",
    "\n",
    "        l4_features=self.conv4(l3_features)#b,48,n\n",
    "        l4_features,l4_index=self.denseconv4(l4_features)\n",
    "        l4_features=torch.cat([l4_features,l3_features],dim=1)#b,168+480=648,n\n",
    "        print(l4_features.shape)\n",
    "\n",
    "        return l4_features\n",
    "    \n",
    "fea_extractor=feature_extraction().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1024])\n",
      "torch.Size([2, 24, 1024])\n",
      "torch.Size([2, 144, 1024])\n",
      "torch.Size([2, 312, 1024])\n",
      "torch.Size([2, 480, 1024])\n",
      "torch.Size([2, 648, 1024])\n",
      "tensor(-1.8792, device='cuda:0', grad_fn=<MinBackward1>) tensor(4.9764, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input=torch.tensor(np.concatenate([pcd_input[np.newaxis,:,0:3],pcd_input2[np.newaxis,:,0:3]])).permute(0,2,1).float().cuda()\n",
    "print(input.shape)\n",
    "\n",
    "feature=fea_extractor(input)\n",
    "print(torch.min(feature),torch.max(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''test the up block'''\n",
    "class up_block(nn.Module):\n",
    "    def __init__(self,up_ratio=4,in_channels=130):\n",
    "        super(up_block,self).__init__()\n",
    "        self.up_ratio=up_ratio\n",
    "        self.conv1=nn.Sequential(\n",
    "            Conv1d(in_channels=in_channels,out_channels=256,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            Conv1d(in_channels=256,out_channels=128,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.grid=Variable(self.gen_grid(up_ratio))\n",
    "        self.attention_unit=attention_unit(in_channels=in_channels)\n",
    "    def forward(self,inputs):\n",
    "        net=inputs #b,128,n\n",
    "        grid=self.grid.requires_grad_(True).cuda()\n",
    "        grid=grid.unsqueeze(0).repeat(net.shape[0],1,net.shape[2])#b,4,2*n\n",
    "        grid=grid.view([net.shape[0],-1,2])#b,4*n,2\n",
    "        grid=grid.permute(0,2,1)#b,2,4*n\n",
    "\n",
    "        net=net.repeat(1,1,self.up_ratio)#b,128,n*4\n",
    "        print(net[1,1,0],net[1,1,1024],net[1,2,2048],net[1,1,3072])\n",
    "        print(grid[1,:,0],grid[1,:,1024],grid[1,:,2048],grid[1,:,3072])\n",
    "        #print(grid.shape)\n",
    "        net=torch.cat([net,grid],dim=1)#b,130,n*4\n",
    "        print(net[:,:,0]-net[:,:,1024])\n",
    "\n",
    "        net=self.attention_unit(net)\n",
    "\n",
    "        net=self.conv1(net)\n",
    "        net=self.conv2(net)\n",
    "\n",
    "        return net\n",
    "\n",
    "\n",
    "    def gen_grid(self,up_ratio):\n",
    "        import math\n",
    "        sqrted=int(math.sqrt(up_ratio))+1\n",
    "        for i in range(1,sqrted+1).__reversed__():\n",
    "            if (up_ratio%i)==0:\n",
    "                num_x=i\n",
    "                num_y=up_ratio//i\n",
    "                break\n",
    "        grid_x=torch.linspace(-0.2,0.2,num_x)\n",
    "        grid_y=torch.linspace(-0.2,0.2,num_y)\n",
    "\n",
    "        x,y=torch.meshgrid([grid_x,grid_y])\n",
    "        grid=torch.stack([x,y],dim=-1)#2,2,2\n",
    "        grid=grid.view([-1,2])#4,2\n",
    "        return grid\n",
    "    \n",
    "class down_block(nn.Module):\n",
    "    def __init__(self,up_ratio=4,in_channels=128):\n",
    "        super(down_block,self).__init__()\n",
    "        self.conv1=nn.Sequential(\n",
    "            Conv2d(in_channels=in_channels,out_channels=256,kernel_size=[up_ratio,1],padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            Conv1d(in_channels=256,out_channels=128,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up_ratio=up_ratio\n",
    "    def forward(self,inputs):\n",
    "        net=inputs#b,128,n*4\n",
    "        net=torch.cat([net[:,:,0:1024].unsqueeze(2),net[:,:,1024:2048].unsqueeze(2),net[:,:,2048:3072].unsqueeze(2),net[:,:,3072:4096].unsqueeze(2)],dim=2)\n",
    "        #net=net.view([inputs.shape[0],inputs.shape[1],self.up_ratio,-1])#b,128,4,n\n",
    "        net=self.conv1(net)#b,256,1,n\n",
    "        net=net.squeeze(2)\n",
    "        net=self.conv2(net)\n",
    "        return net\n",
    "    \n",
    "class up_projection_unit(nn.Module):\n",
    "    def __init__(self,up_ratio=4):\n",
    "        super(up_projection_unit,self).__init__()\n",
    "        self.conv1=nn.Sequential(\n",
    "            Conv1d(in_channels=648,out_channels=128,kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up_block1=up_block(up_ratio=4,in_channels=128+2)\n",
    "        self.up_block2=up_block(up_ratio=4,in_channels=128+2)\n",
    "        self.down_block=down_block(up_ratio=4,in_channels=128)\n",
    "    def forward(self,input):\n",
    "        L=self.conv1(input)#b,128,n\n",
    "        #print(torch.max(L),torch.min(L))\n",
    "        H0=self.up_block1(L)#b,128,n*4\n",
    "        #print(torch.max(H0),torch.min(H0))\n",
    "        L0=self.down_block(H0)#b,128,n\n",
    "\n",
    "        E0=L0-L #b,128,n\n",
    "        H1=self.up_block2(E0)#b,128,4*n\n",
    "        H2=H0+H1 #b,128,4*n\n",
    "        return H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<SelectBackward>) tensor(0., device='cuda:0', grad_fn=<SelectBackward>) tensor(0., device='cuda:0', grad_fn=<SelectBackward>) tensor(0., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-0.2000, -0.2000], device='cuda:0', grad_fn=<SelectBackward>) tensor([-0.2000,  0.2000], device='cuda:0', grad_fn=<SelectBackward>) tensor([ 0.2000, -0.2000], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.2000, 0.2000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.4000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.4000]], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<SelectBackward>) tensor(0., device='cuda:0', grad_fn=<SelectBackward>) tensor(0., device='cuda:0', grad_fn=<SelectBackward>) tensor(0., device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([-0.2000, -0.2000], device='cuda:0', grad_fn=<SelectBackward>) tensor([-0.2000,  0.2000], device='cuda:0', grad_fn=<SelectBackward>) tensor([ 0.2000, -0.2000], device='cuda:0', grad_fn=<SelectBackward>) tensor([0.2000, 0.2000], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.4000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.4000]], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "torch.Size([2, 128, 4096])\n"
     ]
    }
   ],
   "source": [
    "up_projection=up_projection_unit().cuda()\n",
    "feature.unsqueeze(2)\n",
    "#print(torch.max(feature),torch.min(feature))\n",
    "output=up_projection(feature)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
